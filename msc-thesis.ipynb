{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from os import path, mkdir\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import ta\n",
    "from sklearn import preprocessing\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "\n",
    "    def __init__(self, raw_data_dir, processed_data_dir, end_time):\n",
    "        self.raw_data_dir = raw_data_dir\n",
    "        self.processed_data_dir = processed_data_dir\n",
    "        self.end_time = end_time\n",
    "\n",
    "    def extract_data(self, symbol, interval):\n",
    "\n",
    "        # Binance API url\n",
    "        root_binance_url = 'https://api.binance.com/api/v1/klines'\n",
    "        symbol_url = '?symbol='\n",
    "        interval_url = '&interval='\n",
    "        start_time_url = '&startTime='\n",
    "        # limit is max 500 records, max 1200 requests/minute\n",
    "\n",
    "        # check is data file exists\n",
    "        fname = 'binance_' + symbol + '_' + interval + '.json'\n",
    "        fpath = self.raw_data_dir + '/' + fname\n",
    "\n",
    "        if not path.isdir(self.raw_data_dir):\n",
    "            mkdir(self.raw_data_dir)\n",
    "\n",
    "        if not path.isfile(fpath):\n",
    "            print('Downloading data for {}, interval {}...'.format(symbol, interval))\n",
    "\n",
    "            # check for first available timestamp and add 24h since at listing on exchange price varies wildly\n",
    "            url = root_binance_url + symbol_url + symbol + interval_url + interval + start_time_url + '0'\n",
    "            first_timestamp = json.loads(requests.get(url).text)[0][0]  # first timestamp in json\n",
    "            day_in_millis = 86400000\n",
    "            actual_timestamp = first_timestamp + day_in_millis\n",
    "\n",
    "            url = root_binance_url + symbol_url + symbol + interval_url + interval + start_time_url + str(\n",
    "                actual_timestamp)\n",
    "            json_data = json.loads(requests.get(url).text)\n",
    "\n",
    "            # new start time is the previous end timestamp, 500 is the limit/max\n",
    "            start_time = json_data[-1][0]\n",
    "            end_time = self.convert_date(self.end_time, to_timestamp=True)\n",
    "            while start_time < end_time:\n",
    "                url = root_binance_url + symbol_url + symbol + interval_url + interval + start_time_url + str(\n",
    "                    start_time)\n",
    "                data_new = json.loads(requests.get(url).text)\n",
    "                # omit the first element as it is equal to the last on the previous list\n",
    "                json_data = json_data + data_new[1:]\n",
    "                start_time = json_data[-1][0]\n",
    "\n",
    "            # save to disk\n",
    "            with open(fpath, 'w') as f:\n",
    "                json.dump(json_data, f, sort_keys=True, indent=4, ensure_ascii=False)\n",
    "\n",
    "            # return dataframe\n",
    "            df = pd.DataFrame(json_data)\n",
    "            return df, fname\n",
    "\n",
    "        else:\n",
    "            print('Retrieving from file...')\n",
    "            # read from disk into a pandas dataframe\n",
    "            df = pd.read_json(fpath)\n",
    "            return df, fname\n",
    "\n",
    "    def process_data(self, data, fname):\n",
    "\n",
    "        fname = fname.split('binance_')[1]\n",
    "        folder_path = self.raw_data_dir + '/' + self.processed_data_dir\n",
    "        file_path = folder_path + '/' + fname\n",
    "        df = data\n",
    "\n",
    "        if not path.isdir(folder_path):\n",
    "            mkdir(folder_path)\n",
    "\n",
    "        if not path.isfile(file_path):\n",
    "            # remove any rows with null values\n",
    "            df = df.dropna()\n",
    "\n",
    "            # from binance-api-docs: https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md\n",
    "            col_names = ['Open Time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Close Time', 'Quote Asset Volume',\n",
    "                         'Number of trades', 'Taker buy base asset volume', 'Taker buy quote asset volume', 'Ignore']\n",
    "            df.columns = col_names\n",
    "            df.drop(df.columns[8:], axis=1, inplace=True)\n",
    "            # Drop unnecessary columns\n",
    "            col_drop_names = ['Open', 'High', 'Low', 'Volume', 'Close Time']\n",
    "            df.drop(col_drop_names, axis=1, inplace=True)\n",
    "\n",
    "            # Quote Asset Volume is volume in base currency = BTC\n",
    "            df.rename(columns={'Quote Asset Volume': 'Volume'}, inplace=True)\n",
    "\n",
    "            # remove rows after end time\n",
    "            end_time = self.convert_date(self.end_time, to_timestamp=True)\n",
    "            df = df[df['Open Time'] <= end_time]\n",
    "\n",
    "            # sort by ascending date, so that last point represents 15th June 2018\n",
    "            df = df.sort_values(by='Open Time')\n",
    "\n",
    "            # calculate TA indicators\n",
    "            print('Calculating TA indicators...')\n",
    "            df = self.calculate_ta(df)\n",
    "\n",
    "            # remove first 200 elements (MA_200 is nan)\n",
    "            df = df[200:]\n",
    "\n",
    "            # save to disk\n",
    "            with open(file_path, 'w') as f:\n",
    "                # get df to json format\n",
    "                out = df.to_json(orient='records')\n",
    "                f.write(out)\n",
    "\n",
    "        else:\n",
    "            print('Retrieving from file...')\n",
    "            df = pd.read_json(file_path, orient='records')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def load_processed_data(self, fname):\n",
    "        folder_path = self.raw_data_dir + '/' + self.processed_data_dir\n",
    "        file_path = folder_path + '/' + fname + '.json'\n",
    "        return pd.read_json(file_path, orient='records')\n",
    "\n",
    "    def calculate_ta(self, data):\n",
    "\n",
    "        def moving_average(data_col, n):\n",
    "            ma = data_col.rolling(window=n).mean()\n",
    "            ma.fillna(0, inplace=True)\n",
    "            return ma\n",
    "\n",
    "        df = data\n",
    "\n",
    "        # Trend Indicators\n",
    "        # Moving Average (MA)\n",
    "        # df['MA_10'] = moving_average(df['Close'], 10)\n",
    "        df['MA_50'] = moving_average(df['Close'], 50)\n",
    "        df['MA_200'] = moving_average(df['Close'], 200)\n",
    "\n",
    "        # Exponential Moving Average (EMA)\n",
    "        df['EMA'] = ta.ema_slow(df['Close'], n_slow=20, fillna=True)\n",
    "\n",
    "        # Moving Average Convergence Divergence (MACD)\n",
    "        df['MACD'] = ta.macd_diff(df['Close'], n_fast=12, n_slow=26, n_sign=9, fillna=True)\n",
    "\n",
    "        # Momentum Indicators\n",
    "        # Relative Strength Index (RSI)\n",
    "        df['RSI'] = ta.rsi(df['Close'], n=14, fillna=True)\n",
    "\n",
    "        # Volatility Indicators\n",
    "        # Calc volatility manually\n",
    "        # df['Volatility'] = (df['High'] - df['Low']) / df['Open']\n",
    "\n",
    "        # Bollinger Bands (BB)\n",
    "        df['BB_H'] = ta.bollinger_hband_indicator(df['Close'], n=20, ndev=2, fillna=True)\n",
    "        df['BB_L'] = ta.bollinger_lband_indicator(df['Close'], n=20, ndev=2, fillna=True)\n",
    "\n",
    "        # Scaling between -1 and 1 (if crosses 1, else -1)\n",
    "        # df['BB_H'].replace(0, -1, inplace=True)\n",
    "        # df['BB_L'].replace(0, -1, inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def drop_col(self, df, name='Open Time'):\n",
    "        df.drop([name], axis=1, inplace=True)\n",
    "\n",
    "    def convert_date(self, val, to_timestamp):\n",
    "        if to_timestamp:\n",
    "            dt = datetime.strptime(val, '%d.%m.%Y %H:%M:%S')\n",
    "            millis_time = dt.timestamp() * 1000\n",
    "            return int(millis_time)\n",
    "        else:\n",
    "            # time units are milliseconds\n",
    "            date_col = pd.to_datetime(val, unit='ms')\n",
    "            return date_col\n",
    "\n",
    "    def check_if_nan(self, data):\n",
    "\n",
    "        if data.isnull().values.any():\n",
    "            null_cols = data.columns[data.isnull().any()]\n",
    "            data[null_cols].isnull().sum()\n",
    "\n",
    "            # print('Dataset contains null values')\n",
    "            # print(data[data.isnull().any(axis=1)][null_cols].head())\n",
    "\n",
    "            data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    def create_inputs_minmax(self, data, x_win_size=50, y_win_size=1):\n",
    "        # can store 2x in memory compared to float64\n",
    "        tmp_data = data.astype('float32')\n",
    "\n",
    "        self.drop_col(tmp_data, name='Open Time')\n",
    "        self.check_if_nan(tmp_data)\n",
    "\n",
    "        # BB_H, BB_L are in the range [0,1]\n",
    "        # RSI is oscillator [0, 100] --> [0,1]\n",
    "        tmp_data[['RSI']] = tmp_data[['RSI']] / 100\n",
    "\n",
    "        # MaxMin Scaling\n",
    "        scaler = self.maxmin_normalise(tmp_data)\n",
    "\n",
    "        # for col in tmp_data.columns.values:\n",
    "        #     print('{} -- First:'.format(col), tmp_data[col][0], 'Max:', tmp_data[col].max(), 'Min:', tmp_data[col].min())\n",
    "\n",
    "        x_inputs = []\n",
    "        y_inputs = []\n",
    "        i = 0\n",
    "        while (i + x_win_size + y_win_size) <= len(tmp_data):\n",
    "            # e.g. x[0:50] y[50:51]\n",
    "            x_win_data = tmp_data[i: i + x_win_size]\n",
    "            y_win_data = tmp_data['Close'][i + x_win_size: i + x_win_size + y_win_size]\n",
    "\n",
    "            # to numpy array\n",
    "            x_win_arr = np.array(x_win_data)\n",
    "            y_win_arr = np.array(y_win_data)\n",
    "            x_inputs.append(x_win_arr)\n",
    "            y_inputs.append(y_win_arr)\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "        x_inputs = np.array(x_inputs)\n",
    "        y_inputs = np.array(y_inputs)\n",
    "        # reshape for plotting (_,)\n",
    "        y_inputs = np.reshape(y_inputs, (-1,))\n",
    "        print('Shape X:', np.shape(x_inputs), 'Shape Y:', np.shape(y_inputs))\n",
    "\n",
    "        return x_inputs, y_inputs, scaler\n",
    "\n",
    "    def create_inputs_zero_base(self, data, x_win_size=50, y_win_size=1):\n",
    "        # can store 2x in memory compared to float64\n",
    "        tmp_data = data.astype('float32')\n",
    "\n",
    "        self.drop_col(tmp_data, name='Open Time')\n",
    "        self.check_if_nan(tmp_data)\n",
    "\n",
    "        # BB_H, BB_L are in the range [0,1]\n",
    "        # RSI is oscillator [0, 100]\n",
    "        # -- Scale to [0, 2], then shift to [-1, 1] range\n",
    "        tmp_data[['RSI']] = ((tmp_data[['RSI']] / 100) * 2) - 1\n",
    "\n",
    "        x_inputs = []\n",
    "        y_inputs = []\n",
    "        close_bases = []\n",
    "        i = 0\n",
    "        while (i + x_win_size + y_win_size) <= len(tmp_data):\n",
    "            # create a copy to preserve original data\n",
    "            window_data = tmp_data[i: (i + x_win_size + y_win_size)].copy()\n",
    "            window_data, close_base = self.zero_base_normalise(window_data)\n",
    "\n",
    "            # x[0:50] y[50:51]\n",
    "            x_win_data = window_data[: x_win_size]\n",
    "            y_win_data = window_data['Close'].iloc[-1]\n",
    "\n",
    "            # change to numpy array\n",
    "            x_win_arr = np.array(x_win_data)\n",
    "            x_inputs.append(x_win_arr)\n",
    "            y_inputs.append(y_win_data)\n",
    "            close_bases.append(close_base)\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "        x_inputs = np.array(x_inputs)\n",
    "        y_inputs = np.array(y_inputs)\n",
    "        close_bases = np.array(close_bases)\n",
    "\n",
    "        print('Shape X:', np.shape(x_inputs), 'Shape Y:', np.shape(y_inputs))\n",
    "        return x_inputs, y_inputs, close_bases\n",
    "\n",
    "    def create_inputs_reinforcement(self, data, x_win_size=10):\n",
    "        # can store 2x in memory compared to float64\n",
    "        tmp_data = data.astype('float32')\n",
    "\n",
    "        self.drop_col(tmp_data, name='Open Time')\n",
    "        self.check_if_nan(tmp_data)\n",
    "\n",
    "        # BB_H, BB_L are in the range [0,1]\n",
    "        # RSI is oscillator [0, 100] --> [0,1]\n",
    "        tmp_data[['RSI']] = tmp_data[['RSI']] / 100\n",
    "\n",
    "        # MaxMin Scaling\n",
    "        scaler = self.maxmin_normalise(tmp_data)\n",
    "\n",
    "        # 76th input is 0, which can cause division with 0\n",
    "        min_idx = tmp_data['Close'].idxmin()\n",
    "        # replace that element with the previous one\n",
    "        tmp_data['Close'].iloc[min_idx] = tmp_data['Close'].iloc[min_idx - 1]\n",
    "\n",
    "        # no splitting into train and validation subsets\n",
    "        inputs = []\n",
    "        closing_prices = []\n",
    "        i = 0\n",
    "        while (i + x_win_size) <= len(tmp_data):\n",
    "            # e.g. x[0:50]\n",
    "            window = tmp_data[i: (i + x_win_size)]\n",
    "            # price of the window (1st element) at which trade is executed\n",
    "            close_price = window.loc[:, 'Close'].iloc[0]\n",
    "\n",
    "            # to numpy array\n",
    "            win_arr = np.array(window)\n",
    "            inputs.append(win_arr)\n",
    "            closing_prices.append(close_price)\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "        inputs = np.array(inputs)\n",
    "        closing_prices = np.array(closing_prices)\n",
    "\n",
    "        print('Shape Inputs:', np.shape(inputs))\n",
    "        return inputs, closing_prices, scaler\n",
    "\n",
    "    def maxmin_normalise(self, data):\n",
    "\n",
    "        # BB_H, BB_L, RSI are good\n",
    "        cols = [name for name in data.columns if name in ['BB_H', 'BB_L', 'RSI']]\n",
    "        tmp_df = pd.DataFrame()\n",
    "        for col in cols:\n",
    "            tmp_df[col] = data[col]\n",
    "\n",
    "        # rescale to [0, 1]\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "        scaler.fit_transform(data.values)\n",
    "\n",
    "        # replace BB_H, BB_L and RSI with original values\n",
    "        for col in cols:\n",
    "            data[col] = tmp_df[col]\n",
    "\n",
    "        return scaler\n",
    "\n",
    "    def zero_base_normalise(self, data):\n",
    "\n",
    "        # closing price base for inverse scaling\n",
    "        close_base = data.loc[:, 'Close'].iloc[0]\n",
    "\n",
    "        # BB_H, BB_L, RSI are good\n",
    "        norm_cols = [name for name in data.columns if name not in ['BB_H', 'BB_L', 'RSI']]\n",
    "        for col in norm_cols:\n",
    "            # normalise against the 1st element for each window\n",
    "            tmp_base = data.loc[:, col].iloc[0]\n",
    "            data.loc[:, col] = (data.loc[:, col] / tmp_base) - 1\n",
    "\n",
    "        return data, close_base\n",
    "\n",
    "    def split_train_test(self, df, train_set_size=0.8):\n",
    "        train_set = df[:int(train_set_size * len(df))]\n",
    "        test_set = df[int(train_set_size * len(df)):]\n",
    "        return train_set, test_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from plotly import tools as pytls\n",
    "\n",
    "\n",
    "def plotly_layout(plot_title, ty1, ty2):\n",
    "    layout = go.Layout(\n",
    "        title=plot_title,\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace'\n",
    "        ),\n",
    "        legend=dict(orientation='h'),\n",
    "        xaxis=dict(type='date'),\n",
    "        yaxis=dict(\n",
    "            domain=[0, 0.3],\n",
    "            title=ty1,\n",
    "            titlefont=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=16\n",
    "            ),\n",
    "            hoverformat='.4f',\n",
    "            tickformat='.f'\n",
    "        ),\n",
    "        yaxis2=dict(\n",
    "            domain=[0.4, 1],\n",
    "            title=ty2,\n",
    "            titlefont=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=16\n",
    "            ),\n",
    "            hoverformat='.8f',\n",
    "            tickformat='.6f'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return layout\n",
    "\n",
    "\n",
    "def plotly_individual(data, tag):\n",
    "    py.init_notebook_mode(connected=True)\n",
    "\n",
    "    trace_price = go.Scatter(\n",
    "        x=data['Open Time'],\n",
    "        y=data['Close'],\n",
    "        name=tag[:3]\n",
    "    )\n",
    "\n",
    "    trace_volume = go.Scatter(\n",
    "        x=data['Open Time'],\n",
    "        y=data['Volume'],\n",
    "        xaxis='x',\n",
    "        yaxis='y2',\n",
    "        name=tag[:3]\n",
    "    )\n",
    "\n",
    "    ty1 = 'Volume [BTC]'\n",
    "    ty2 = 'Closing Price [BTC]'\n",
    "    layout = plotly_layout(tag, ty1, ty2)\n",
    "\n",
    "    fig = pytls.make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.01)\n",
    "    fig.append_trace(trace_price, 2, 1)\n",
    "    fig.append_trace(trace_volume, 1, 1)\n",
    "\n",
    "    fig = go.Figure(fig, layout=layout)\n",
    "    py.iplot(fig, filename=tag)\n",
    "\n",
    "\n",
    "def plotly_all(data, labels, normalise=False):\n",
    "    py.init_notebook_mode(connected=True)\n",
    "\n",
    "    fig = pytls.make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.01)\n",
    "    plot_title, ty1, ty2 = '', '', ''\n",
    "\n",
    "    colours = ['#1F77B4', '#B2182B', '#FF7F0E']\n",
    "    for i, key in enumerate(labels):\n",
    "        price_data = data[key]['Close']\n",
    "        vol_data = data[key]['Volume']\n",
    "        open_times = data[key]['Open Time']\n",
    "        plot_title = 'Comparison of Closing Prices and Volume of EOS, TRX and ONT'\n",
    "        ty1 = 'Volume [BTC]'\n",
    "        ty2 = 'Closing Price [BTC]'\n",
    "\n",
    "        if normalise:\n",
    "            price_data = plot_scale(price_data)\n",
    "            vol_data = plot_scale(vol_data)\n",
    "            plot_title = 'Comparison of Normalised Closing Prices and Volume of EOS, TRX and ONT'\n",
    "            ty1 = ty1[:-6]\n",
    "            ty2 = ty2[:-6]\n",
    "\n",
    "        tmp_price = go.Scatter(\n",
    "            x=open_times,\n",
    "            y=price_data,\n",
    "            name=key[:3],\n",
    "            line=dict(color=colours[i])\n",
    "        )\n",
    "        fig.append_trace(tmp_price, 2, 1)\n",
    "\n",
    "        tmp_vol = go.Scatter(\n",
    "            x=open_times,\n",
    "            y=vol_data,\n",
    "            xaxis='x',\n",
    "            yaxis='y2',\n",
    "            name=key[:3],\n",
    "            line=dict(color=colours[i]),\n",
    "            showlegend=False\n",
    "        )\n",
    "        fig.append_trace(tmp_vol, 1, 1)\n",
    "\n",
    "    layout = plotly_layout(plot_title, ty1, ty2)\n",
    "\n",
    "    fig = go.Figure(fig, layout=layout)\n",
    "    py.iplot(fig, filename='all-together')\n",
    "\n",
    "\n",
    "def plot_individual(data, tag):\n",
    "    p = PrepareData()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
    "    fig.suptitle(tag, fontsize=16)\n",
    "\n",
    "    date_col = p.convert_date(data['Open Time'], False)\n",
    "    ax1.plot(date_col, data['Close'])\n",
    "    ax1.grid(True)\n",
    "    ax1.set_ylabel('Closing Price [BTC]', fontsize=12)\n",
    "    ax1.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "    ax1.locator_params(axis='y', nbins=12)\n",
    "    ax1.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.6f'))\n",
    "\n",
    "    ax2.plot(date_col, data['Volume'])\n",
    "    ax2.grid(True)\n",
    "    ax2.set_ylabel('Volume [BTC]', fontsize=12)\n",
    "    ax2.locator_params(axis='y', nbins=4)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_all(data):\n",
    "    p = PrepareData()\n",
    "\n",
    "    eos = data['EOSBTC_30m']\n",
    "    eos_np = plot_scale(eos['Close'])\n",
    "    trx = data['TRXBTC_30m']\n",
    "    trx_np = plot_scale(trx['Close'])\n",
    "    ont = data['ONTBTC_30m']\n",
    "    ont_np = plot_scale(ont['Close'])\n",
    "    date_eos = p.convert_date(eos['Open Time'], False)\n",
    "    date_trx = p.convert_date(trx['Open Time'], False)\n",
    "    date_ont = p.convert_date(ont['Open Time'], False)\n",
    "    plt.plot(date_eos, eos_np, 'r', date_trx, trx_np, 'b', date_ont, ont_np, 'g')\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.ylabel('Normalised Closing Price', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_scale(data_col):\n",
    "    # min-max scaling (values between 0 and 1)\n",
    "    scaled = (data_col - min(data_col)) / (max(data_col) - min(data_col))\n",
    "    return scaled\n",
    "\n",
    "\n",
    "def plotly_results_layout(plot_title, x_label, y_label):\n",
    "    layout = go.Layout(\n",
    "        title=plot_title,\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace'\n",
    "        ),\n",
    "        legend=dict(orientation='h'),\n",
    "        xaxis=dict(\n",
    "            title=x_label,\n",
    "            titlefont=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=16\n",
    "            )\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=y_label,\n",
    "            titlefont=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=16\n",
    "            ),\n",
    "            hoverformat='.8f',\n",
    "            tickformat='.6f'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return layout\n",
    "\n",
    "\n",
    "def plotly_loss(history, tag):\n",
    "    py.init_notebook_mode(connected=True)\n",
    "\n",
    "    # Plot Loss\n",
    "    plot_title = 'Model Loss {}'.format(tag)\n",
    "    ty1 = 'Number of Epochs'\n",
    "    ty2 = 'Loss'\n",
    "\n",
    "    trace_loss = go.Scatter(\n",
    "        x=history.epoch,\n",
    "        y=history.history['loss'],\n",
    "        name='Loss'\n",
    "    )\n",
    "\n",
    "    trace_val_loss = go.Scatter(\n",
    "        x=history.epoch,\n",
    "        y=history.history['val_loss'],\n",
    "        name='Validation Loss'\n",
    "    )\n",
    "\n",
    "    layout = plotly_results_layout(plot_title, ty1, ty2)\n",
    "    data = [trace_loss, trace_val_loss]\n",
    "\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    py.iplot(fig, filename='model-loss')\n",
    "\n",
    "\n",
    "def plotly_prediction(predict_df, target, tag):\n",
    "    py.init_notebook_mode(connected=True)\n",
    "\n",
    "    # Plot Loss\n",
    "    plot_title = 'Prediction {}'.format(tag)\n",
    "    ty1 = 'Date'\n",
    "    ty2 = 'Price'\n",
    "\n",
    "    trace_target = go.Scatter(\n",
    "        x=predict_df['Open Time'],\n",
    "        y=target,\n",
    "        name='Actual'\n",
    "    )\n",
    "\n",
    "    trace_predicted = go.Scatter(\n",
    "        x=predict_df['Open Time'],\n",
    "        y=predict_df['Results'],\n",
    "        name='Predicted'\n",
    "    )\n",
    "\n",
    "    layout = plotly_results_layout(plot_title, ty1, ty2)\n",
    "    # process timestamps as date\n",
    "    layout.update(xaxis=dict(type='date'))\n",
    "\n",
    "    data = [trace_target, trace_predicted]\n",
    "\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    py.iplot(fig, filename='prediction-results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        # load configuration file\n",
    "        configs = json.loads(open('config.json').read())\n",
    "\n",
    "        self.neurons = configs['model']['neurons']  # number of hidden units in the LSTM layer\n",
    "        self.activation_function = configs['model']['activation_function']\n",
    "        self.loss_function = configs['model']['loss_function']  # loss function for calculating the gradient\n",
    "        self.optimizer = configs['model']['optimizer']  # optimizer for applying gradient descent\n",
    "        self.dropout = configs['model']['dropout']  # dropout rate used after each LSTM layer to avoid overfitting\n",
    "        self.model_dir = configs['model']['models_dir']\n",
    "        self.dqn_dir = configs['model']['dqn_dir']\n",
    "\n",
    "    def build_network(self, shape, output_size, reinforcement):\n",
    "        # start stacking layers\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(\n",
    "            self.neurons,\n",
    "            # return_sequences=True,\n",
    "            input_shape=(shape[1], shape[2]),  # Shape X (1105, 50, 9), Shape Y (1105, )\n",
    "            activation=self.activation_function\n",
    "        ))\n",
    "        model.add(Dropout(self.dropout))\n",
    "\n",
    "        # model.add(LSTM(neurons, activation=activation_function))\n",
    "        # model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(units=output_size))  # 1 (price) or 3 (hold, long, short)\n",
    "        if not reinforcement:\n",
    "            model.add(Activation(self.activation_function))\n",
    "        else:\n",
    "            model.add(Activation('linear'))\n",
    "\n",
    "        model.compile(loss=self.loss_function, optimizer=self.optimizer)\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "def save_network(model, model_name, reinforcement):\n",
    "    # load configuration file\n",
    "    configs = json.loads(open('config.json').read())\n",
    "    model_dir = configs['model']['models_dir']\n",
    "    dqn_dir = configs['model']['dqn_dir']\n",
    "\n",
    "    fpath = dqn_dir if reinforcement else model_dir\n",
    "    if not path.isdir(fpath):\n",
    "        mkdir(fpath)\n",
    "\n",
    "    filename = fpath + '/' + model_name + '.h5'\n",
    "    model.save(filename)\n",
    "\n",
    "\n",
    "def load_network(model_name):\n",
    "    # load configuration file\n",
    "    configs = json.loads(open('config.json').read())\n",
    "    model_dir = configs['model']['models_dir']\n",
    "    dqn_dir = configs['model']['dqn_dir']\n",
    "\n",
    "    if not path.isfile(model_dir + '/' + model_name + '.h5'):\n",
    "        print('Model {} does not exist...'.format(model_name))\n",
    "    else:\n",
    "        return load_model(model_dir + '/' + model_name + '.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load configuration file\n",
    "configs = json.loads(open('config.json').read())\n",
    "\n",
    "symbols = configs['binance']['symbols']\n",
    "intervals = configs['binance']['intervals']  # 1d does not give enough data\n",
    "end_time = configs['binance']['end_time']\n",
    "x_window_size = configs['data']['x_window_size']\n",
    "y_window_size = configs['data']['y_window_size']\n",
    "input_scaling = configs['data']['input_scaling']\n",
    "train_set_size = configs['data']['train_set_size']\n",
    "raw_data_dir = configs['data']['raw_data_dir']\n",
    "processed_data_dir = configs['data']['processed_data_dir']\n",
    "batch_size = configs['model']['batch_size']  # default 32\n",
    "epochs = configs['model']['epochs']\n",
    "\n",
    "p = PrepareData(raw_data_dir, processed_data_dir, end_time)\n",
    "\n",
    "# Extract and Process Data\n",
    "df_col = {}\n",
    "for sym in symbols:\n",
    "    for t in intervals:\n",
    "        print('\\nGetting data for {}, interval {}'.format(sym, t))\n",
    "        df, fname = p.extract_data(sym, t)\n",
    "\n",
    "        print('\\nProcessing data for {}, interval {}'.format(sym, t))\n",
    "        df_col[sym + '_' + t] = p.process_data(df, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_raw_data(df_col):\n",
    "    # Matplotlib plot\n",
    "    key = 'EOSBTC_30m'\n",
    "    plot_individual(df_col[key], key)\n",
    "    key = 'TRXBTC_30m'\n",
    "    plot_individual(df_col[key], key)\n",
    "    key = 'ONTBTC_30m'\n",
    "    plot_individual(df_col[key], key)\n",
    "    plot_all(df_col)\n",
    "\n",
    "    # Plotly plot\n",
    "    key = 'EOSBTC_30m'\n",
    "    plotly_individual(df_col[key], key)\n",
    "    key = 'TRXBTC_30m'\n",
    "    plotly_individual(df_col[key], key)\n",
    "    key = 'ONTBTC_30m'\n",
    "    plotly_individual(df_col[key], key)\n",
    "    labels = ['EOSBTC_30m', 'TRXBTC_30m', 'ONTBTC_30m']\n",
    "    plotly_all(df_col, labels, normalise=True)\n",
    "    \n",
    "    \n",
    "# plot_raw_data(df_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with \"label\" chart from here onward\n",
    "label = configs['data']['chart']\n",
    "chart = df_col[label]\n",
    "\n",
    "# Split to training and testing data\n",
    "train_set, test_set = p.split_train_test(chart, train_set_size=train_set_size)\n",
    "\n",
    "if input_scaling == 'minmax':\n",
    "    # creates training inputs and outputs (input data and labels for supervised learning)\n",
    "    print('Generating training inputs and lables (X_train, Y_train)...')\n",
    "    X_train, Y_train, train_scaler = p.create_inputs_minmax(train_set, x_win_size=x_window_size, y_win_size=y_window_size)\n",
    "    # creates testing inputs and outputs (validation set to check if the NN is overfitting)\n",
    "    print('Generating testing inputs and lables (X_test, Y_test)...')\n",
    "    X_test, Y_test, test_scaler = p.create_inputs_minmax(test_set, x_win_size=x_window_size, y_win_size=y_window_size)\n",
    "\n",
    "    model_name = label + '_e' + str(epochs) + '_maxmin'\n",
    "\n",
    "else:\n",
    "    # creates training inputs and outputs (input data and labels for supervised learning)\n",
    "    print('Generating training inputs and lables (X_train, Y_train)...')\n",
    "    X_train, Y_train, train_bases = p.create_inputs_zero_base(train_set, x_win_size=x_window_size, y_win_size=y_window_size)\n",
    "    # creates testing inputs and outputs (validation set to check if the NN is overfitting)\n",
    "    print('Generating testing inputs and lables (X_test, Y_test)...')\n",
    "    X_test, Y_test, test_bases = p.create_inputs_zero_base(test_set, x_win_size=x_window_size, y_win_size=y_window_size)\n",
    "\n",
    "    model_name = label + '_e' + str(epochs) + '_zerobase'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # clean up the memory\n",
    "gc.collect()\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(202)\n",
    "\n",
    "# create model architecture\n",
    "lstm = Model()\n",
    "lstm_model = lstm.build_network(shape=X_train.shape, output_size=1, reinforcement=False)\n",
    "\n",
    "history = lstm_model.fit(X_train, Y_train, epochs=epochs, shuffle=True, verbose=2, batch_size=batch_size, validation_data=(X_test, Y_test))\n",
    "\n",
    "# save the model to a file\n",
    "save_network(lstm_model, model_name, reinforcement=False)\n",
    "# plot model loss\n",
    "plotly_loss(history, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def one_step_prediction(X_test, Y_test, chart, label, lstm_model):\n",
    "    predictions = lstm_model.predict(X_test, verbose=2)\n",
    "\n",
    "    # keep date for plotting\n",
    "    size_test_set = X_test.shape[0]\n",
    "    predict_df = pd.DataFrame(chart['Open Time'].iloc[-size_test_set:])\n",
    "\n",
    "    # need to reset the index to start from 0\n",
    "    predict_df.index = range(len(predict_df))\n",
    "    predict_df['Results'] = pd.DataFrame(predictions)\n",
    "\n",
    "    # plot prediction results\n",
    "    plotly_prediction(predict_df, Y_test, label)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# single_step_p = one_step_prediction(X_test, Y_test, chart, label, lstm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def multi_step_prediction(lstm_model, p, inputs, x_window_size, y_window_size, input_scaling):\n",
    "    # 200 needed for MA_200 + 50 for window length\n",
    "    test_set = inputs[:250].copy()\n",
    "    # need to reset the index to start from 0\n",
    "    test_set.index = range(len(test_set))\n",
    "\n",
    "    predictions = []\n",
    "    for i in range(50):\n",
    "        print('Iteration:', i + 1)\n",
    "\n",
    "        if input_scaling == 'maxmin':\n",
    "            X_test, Y_test, scaler = p.create_inputs_minmax(test_set, x_win_size=x_window_size, y_win_size=y_window_size)\n",
    "        else:\n",
    "            X_test, Y_test, close_bases = p.create_inputs_zero_base(test_set, x_win_size=x_window_size, y_win_size=y_window_size)\n",
    "\n",
    "        # only do prediction on the last window\n",
    "        last = X_test[-1]\n",
    "        last = np.reshape(last, (1, X_test.shape[1], X_test.shape[2]))\n",
    "        out = lstm_model.predict(last, verbose=2)\n",
    "\n",
    "        # ndarray with same number of features required for inverse scaling\n",
    "        tmp_nd = np.zeros((len(out), X_test.shape[2]))\n",
    "        # get Close column index (has to match for inverse scaling)\n",
    "        close_ix = test_set.columns.get_loc('Close')\n",
    "        # assign LSTM output to that column\n",
    "        tmp_nd[:, close_ix] = out\n",
    "\n",
    "        # perform inverse scaling and save the correct col value to out_inv\n",
    "        if input_scaling == 'maxmin':\n",
    "            out_inv = scaler.inverse_transform(tmp_nd)[:, close_ix]\n",
    "        else:\n",
    "            out_inv = (out + 1) * close_bases[-1]\n",
    "\n",
    "        # change to float\n",
    "        out_inv = float(out_inv)\n",
    "\n",
    "        predictions.append(out_inv)\n",
    "        print('Predicted:', out_inv, 'True:', inputs.loc[:, 'Close'].iloc[i + 250])\n",
    "\n",
    "        # add output back to the initial df\n",
    "        test_set = test_set.append({'Close': out_inv}, ignore_index=True)\n",
    "\n",
    "        # calculate indicators\n",
    "        test_set = p.calculate_ta(test_set)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# multi_step_p = multi_step_prediction(lstm_model, p, test_set, x_window_size, y_window_size, input_scaling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def evaluate():\n",
    "    rmse = sqrt(mean_squared_error(lstm_out, Y_test))\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "\n",
    "    # load and plot some other networks\n",
    "    loaded_label = 'EOSBTC_30m_150'\n",
    "    loaded_model = load_network(loaded_label)\n",
    "\n",
    "    # evaluate loaded model on test data\n",
    "    loaded_model.compile(loss=loss_function, optimizer=optimizer, metrics=['mae'])\n",
    "    score = loaded_model.evaluate(X_train, Y_train, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "\n",
    "    # load and plot some other networks\n",
    "    loaded_label = 'EOSBTC_4h_100'\n",
    "    loaded_model = load_network(loaded_label)\n",
    "\n",
    "    # evaluate loaded model on test data\n",
    "    loaded_model.compile(loss=loss_function, optimizer=optimizer, metrics=['mae'])\n",
    "    score = loaded_model.evaluate(X_train, Y_train, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "\n",
    "    # load and plot some other networks\n",
    "    loaded_label = 'EOSBTC_1h_150'\n",
    "    loaded_model = load_network(loaded_label)\n",
    "\n",
    "    # evaluate loaded model on test data\n",
    "    loaded_model.compile(loss=loss_function, optimizer=optimizer, metrics=['mae'])\n",
    "    score = loaded_model.evaluate(X_train, Y_train, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "    \n",
    "\n",
    "# evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_shape):\n",
    "        # load configuration file\n",
    "        configs = json.loads(open('config.json').read())\n",
    "        self.action_size = configs['reinforcement']['action_size']  # hold, long. short\n",
    "        self.gamma = configs['reinforcement']['gamma']  # reward discount rate\n",
    "        self.epsilon = configs['reinforcement']['epsilon']  # exploration rate\n",
    "        self.epsilon_min = configs['reinforcement']['epsilon_min']\n",
    "        self.epsilon_decay = configs['reinforcement']['epsilon_decay']\n",
    "        self.batch_size = configs['reinforcement']['batch_size']\n",
    "\n",
    "        self.memory = deque(maxlen=2000)  # deque automatically removes oldest memory once the maxlen is reached\n",
    "        self.state_shape = state_shape[1:]  # (_, window_len, features)\n",
    "        self.inventory = []\n",
    "\n",
    "        self.model = Model().build_network(shape=state_shape, output_size=self.action_size, reinforcement=True)\n",
    "        self.history = 0\n",
    "\n",
    "    def compute_action(self, state, evaluation):\n",
    "        # get random action based on epsilon for exploration\n",
    "        if not evaluation and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        # returns a list\n",
    "        options = self.model.predict(state)\n",
    "\n",
    "        # returns the index of the max value in this state [_, _, _]\n",
    "        return np.argmax(options[0])  # returns action\n",
    "\n",
    "    def calculate_reward(self, action, price, long, short):\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1:  # buy\n",
    "\n",
    "            # if new trade\n",
    "            if not long and not short:\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # if already short\n",
    "            elif short:\n",
    "                # close short, open long\n",
    "                sold_price = self.inventory.pop()\n",
    "                # short is inverse\n",
    "                reward = (sold_price - price) / sold_price  # reward is percentage\n",
    "                # reward = sold_price - price\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # set new trade\n",
    "            long = True\n",
    "            short = False\n",
    "\n",
    "        elif action == 2:  # sell\n",
    "\n",
    "            # if new trade\n",
    "            if not long and not short:\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # if already long\n",
    "            elif long:\n",
    "                # close long, open short\n",
    "                bought_price = self.inventory.pop()\n",
    "                reward = (price - bought_price) / bought_price\n",
    "                # reward = price - bought_price\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # set new trade\n",
    "            long = False\n",
    "            short = True\n",
    "\n",
    "        return reward, long, short\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def experience_replay(self, batch_size):\n",
    "\n",
    "        # fill minibatch with random states from the memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "\n",
    "            if not done:\n",
    "                # get max (highest probability) action\n",
    "                tmp = np.amax(self.model.predict(next_state)[0])\n",
    "                target = reward + self.gamma * tmp\n",
    "\n",
    "            # predict next action\n",
    "            target_f = self.model.predict(state)\n",
    "            # target is [[_, _, _]]\n",
    "            target_f[0][action] = target\n",
    "            # train the model towards updated prediction\n",
    "            history = self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            # save loss history\n",
    "            self.history = history\n",
    "\n",
    "        # update exploration/exploitation rate\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def main():\n",
    "    # load configuration file\n",
    "    configs = json.loads(open('config.json').read())\n",
    "\n",
    "    raw_data_dir = configs['data']['raw_data_dir']\n",
    "    processed_data_dir = configs['data']['processed_data_dir']\n",
    "    end_time = configs['binance']['end_time']\n",
    "    train_set_size = configs['data']['train_set_size']\n",
    "    label = configs['data']['chart']\n",
    "\n",
    "    window_size = configs['reinforcement']['window_size']\n",
    "\n",
    "    p = PrepareData(raw_data_dir, processed_data_dir, end_time)\n",
    "\n",
    "    df = p.load_processed_data(label)\n",
    "    train_set, test_set = p.split_train_test(df, train_set_size=train_set_size)\n",
    "\n",
    "    print('Generating inputs ...')\n",
    "    inputs, closing_prices, scaler = p.create_inputs_reinforcement(train_set, x_win_size=window_size)\n",
    "\n",
    "    agent = DQNAgent(np.shape(inputs))\n",
    "    evaluation = False  # switch on at testing stage\n",
    "    commission = 0.025\n",
    "\n",
    "    episodes = configs['reinforcement']['episodes']\n",
    "    rewards = []\n",
    "    losses = []\n",
    "    elapsed_times = [] \n",
    "\n",
    "    time_steps = len(inputs)\n",
    "    for e in range(episodes):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # reset the environment (first window [0:10])\n",
    "        state = inputs[0]\n",
    "        state = np.reshape(state, (1, agent.state_shape[0], agent.state_shape[1]))  # (1, 10, 9)\n",
    "        total_reward = 100  # in percentage\n",
    "        long = False\n",
    "        short = False\n",
    "        action = 0\n",
    "        agent.inventory = []\n",
    "        done = False\n",
    "\n",
    "        # maybe clear memory every episode\n",
    "\n",
    "        # sparse rewards, based on exited trade - next, implement them on a daily basis\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            print('Step {}/{}'.format(t, time_steps))\n",
    "            print('Agent\\'s inventory:', agent.inventory, 'Action:', action, 'Total reward:', total_reward)\n",
    "            # print('Reward', reward)\n",
    "\n",
    "            # compute action based on current state\n",
    "            action = agent.compute_action(state, evaluation)\n",
    "\n",
    "            # calculate reward\n",
    "            reward, long, short = agent.calculate_reward(action, closing_prices[t], long, short)\n",
    "            total_reward = total_reward + reward\n",
    "\n",
    "            if total_reward < 0 or t == time_steps - 1:\n",
    "                done = True\n",
    "\n",
    "            if done:\n",
    "                end_time = time.time()\n",
    "                \n",
    "                if total_reward < 0:\n",
    "                    print('REKT')\n",
    "                    \n",
    "                rewards.append(total_reward)\n",
    "                losses.append(agent.history.history)\n",
    "                elapsed_times.append(end_time - start_time)\n",
    "                print(\"--------------------------------\")\n",
    "                print(\"Episode: {}/{}, score: {}, training loss: {}\".format(e, episodes, total_reward, agent.history.history))\n",
    "                print(\"--------------------------------\")\n",
    "                break\n",
    "\n",
    "            # get next state\n",
    "            next_state = inputs[t + 1]\n",
    "            next_state = np.reshape(next_state, state.shape)\n",
    "\n",
    "            # store in memory\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            # recall sometimes\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                agent.experience_replay(agent.batch_size)\n",
    "\n",
    "            if e % 1 == 0 and not e == 0:\n",
    "                save_network(agent.model, 'model_ep{}'.format(e), reinforcement=True)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
