{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from os import path, mkdir, makedirs\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import ta\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "\n",
    "    def __init__(self):\n",
    "        # load configuration file\n",
    "        configs = json.loads(open('config.json').read())\n",
    "\n",
    "        self.raw_data_dir = configs['data']['raw_data_dir']\n",
    "        self.processed_data_dir = configs['data']['processed_data_dir']\n",
    "        self.end_time = configs['binance']['end_time']\n",
    "        self.symbols = configs['binance']['symbols']\n",
    "        self.intervals = configs['binance']['intervals']\n",
    "\n",
    "    def extract_data(self, symbol, interval):\n",
    "\n",
    "        # Binance API url\n",
    "        root_binance_url = 'https://api.binance.com/api/v1/klines'\n",
    "        symbol_url = '?symbol='\n",
    "        interval_url = '&interval='\n",
    "        start_time_url = '&startTime='\n",
    "        # limit is max 500 records, max 1200 requests/minute\n",
    "\n",
    "        # check is data file exists\n",
    "        fname = 'binance_' + symbol + '_' + interval + '.json'\n",
    "        fpath = path.join(self.raw_data_dir, fname)\n",
    "        if not path.isdir(self.raw_data_dir):\n",
    "            mkdir(self.raw_data_dir)\n",
    "\n",
    "        if not path.isfile(fpath):\n",
    "            print('Downloading data for {}, interval {}...'.format(symbol, interval))\n",
    "\n",
    "            # check for first available timestamp and add 24h since at listing on exchange price varies wildly\n",
    "            url = root_binance_url + symbol_url + symbol + interval_url + interval + start_time_url + '0'\n",
    "            first_timestamp = json.loads(requests.get(url).text)[0][0]  # first timestamp in json\n",
    "            day_in_millis = 86400000\n",
    "            actual_timestamp = first_timestamp + day_in_millis\n",
    "\n",
    "            url = root_binance_url + symbol_url + symbol + interval_url + interval + start_time_url + str(\n",
    "                actual_timestamp)\n",
    "            json_data = json.loads(requests.get(url).text)\n",
    "\n",
    "            # new start time is the previous end timestamp, 500 is the limit/max\n",
    "            start_time = json_data[-1][0]\n",
    "            end_time = self.convert_date(self.end_time, to_timestamp=True)\n",
    "            while start_time < end_time:\n",
    "                url = root_binance_url + symbol_url + symbol + interval_url + interval + start_time_url + str(\n",
    "                    start_time)\n",
    "                data_new = json.loads(requests.get(url).text)\n",
    "                # omit the first element as it is equal to the last on the previous list\n",
    "                json_data = json_data + data_new[1:]\n",
    "                start_time = json_data[-1][0]\n",
    "\n",
    "            # save to disk\n",
    "            with open(fpath, 'w') as f:\n",
    "                json.dump(json_data, f, sort_keys=True, indent=4, ensure_ascii=False)\n",
    "\n",
    "            # return dataframe\n",
    "            df = pd.DataFrame(json_data)\n",
    "            return df, fname\n",
    "\n",
    "        else:\n",
    "            print('Retrieving from file...')\n",
    "            # read from disk into a pandas dataframe\n",
    "            df = pd.read_json(fpath)\n",
    "            return df, fname\n",
    "\n",
    "    def process_data(self, data, fname):\n",
    "\n",
    "        df = data\n",
    "\n",
    "        fname = fname.split('binance_')[1]\n",
    "        fpath = path.join(self.raw_data_dir, self.processed_data_dir)\n",
    "        file_path = fpath + '/' + fname\n",
    "        if not path.isdir(fpath):\n",
    "            mkdir(fpath)\n",
    "\n",
    "        if not path.isfile(file_path):\n",
    "            # remove any rows with null values\n",
    "            df = df.dropna()\n",
    "\n",
    "            # from binance-api-docs: https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md\n",
    "            col_names = ['Open Time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Close Time', 'Quote Asset Volume',\n",
    "                         'Number of trades', 'Taker buy base asset volume', 'Taker buy quote asset volume', 'Ignore']\n",
    "            df.columns = col_names\n",
    "            df.drop(df.columns[8:], axis=1, inplace=True)\n",
    "            # Drop unnecessary columns\n",
    "            col_drop_names = ['Open', 'High', 'Low', 'Volume', 'Close Time']\n",
    "            df.drop(col_drop_names, axis=1, inplace=True)\n",
    "\n",
    "            # Quote Asset Volume is volume in base currency = BTC\n",
    "            df.rename(columns={'Quote Asset Volume': 'Volume'}, inplace=True)\n",
    "\n",
    "            # remove rows after end time\n",
    "            end_time = self.convert_date(self.end_time, to_timestamp=True)\n",
    "            df = df[df['Open Time'] <= end_time]\n",
    "\n",
    "            # sort by ascending date\n",
    "            df = df.sort_values(by='Open Time')\n",
    "\n",
    "            print('Calculating TA indicators...')\n",
    "            df = self.calculate_ta(df)\n",
    "\n",
    "            # remove first 200 elements (MA_200 is nan)\n",
    "            df = df[200:]\n",
    "\n",
    "            # save to disk\n",
    "            with open(file_path, 'w') as f:\n",
    "                out = df.to_json(orient='records')\n",
    "                f.write(out)\n",
    "\n",
    "        else:\n",
    "            print('Retrieving from file...')\n",
    "            df = pd.read_json(file_path, orient='records')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def load_processed_data(self, fname):\n",
    "        folder_path = self.raw_data_dir + '/' + self.processed_data_dir\n",
    "        file_path = folder_path + '/' + fname + '.json'\n",
    "        return pd.read_json(file_path, orient='records')\n",
    "\n",
    "    def calculate_ta(self, data):\n",
    "\n",
    "        def moving_average(data_col, n):\n",
    "            ma = data_col.rolling(window=n).mean()\n",
    "            ma.fillna(0, inplace=True)\n",
    "            return ma\n",
    "\n",
    "        df = data\n",
    "\n",
    "        # Trend Indicators\n",
    "        # Moving Average (MA)\n",
    "        df['MA_50'] = moving_average(df['Close'], 50)\n",
    "        df['MA_200'] = moving_average(df['Close'], 200)\n",
    "\n",
    "        # Exponential Moving Average (EMA)\n",
    "        df['EMA'] = ta.ema_slow(df['Close'], n_slow=20, fillna=True)\n",
    "\n",
    "        # Moving Average Convergence Divergence (MACD)\n",
    "        df['MACD'] = ta.macd_diff(df['Close'], n_fast=12, n_slow=26, n_sign=9, fillna=True)\n",
    "\n",
    "        # Momentum Indicators\n",
    "        # Relative Strength Index (RSI)\n",
    "        df['RSI'] = ta.rsi(df['Close'], n=14, fillna=True)\n",
    "\n",
    "        # Volatility Indicators\n",
    "        # Bollinger Bands (BB)\n",
    "        df['BB_H'] = ta.bollinger_hband_indicator(df['Close'], n=20, ndev=2, fillna=True)\n",
    "        df['BB_L'] = ta.bollinger_lband_indicator(df['Close'], n=20, ndev=2, fillna=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def drop_col(self, df, name='Open Time'):\n",
    "        df.drop([name], axis=1, inplace=True)\n",
    "\n",
    "    def convert_date(self, val, to_timestamp):\n",
    "        if to_timestamp:\n",
    "            dt = datetime.strptime(val, '%d.%m.%Y %H:%M:%S')\n",
    "            millis_time = dt.timestamp() * 1000\n",
    "            return int(millis_time)\n",
    "        else:\n",
    "            # time units are milliseconds\n",
    "            date_col = pd.to_datetime(val, unit='ms')\n",
    "            return date_col\n",
    "\n",
    "    def check_if_nan(self, data):\n",
    "\n",
    "        if data.isnull().values.any():\n",
    "            null_cols = data.columns[data.isnull().any()]\n",
    "            data[null_cols].isnull().sum()\n",
    "\n",
    "            # print('Dataset contains null values')\n",
    "            # print(data[data.isnull().any(axis=1)][null_cols].head())\n",
    "\n",
    "            data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    def create_inputs_minmax(self, data, x_win_size=50, y_win_size=1):\n",
    "        # can store 2x in memory compared to float64\n",
    "        tmp_data = data.astype('float32')\n",
    "\n",
    "        self.drop_col(tmp_data, name='Open Time')\n",
    "        self.check_if_nan(tmp_data)\n",
    "\n",
    "        # BB_H, BB_L are in the range [0,1]\n",
    "        # RSI is oscillator [0, 100] --> [0,1]\n",
    "        tmp_data[['RSI']] = tmp_data[['RSI']] / 100\n",
    "\n",
    "        scaler = self.scale_minmax(tmp_data)\n",
    "\n",
    "        x_inputs = []\n",
    "        y_inputs = []\n",
    "        i = 0\n",
    "        while (i + x_win_size + y_win_size) <= len(tmp_data):\n",
    "            # e.g. x[0:50] y[50:51]\n",
    "            x_win_data = tmp_data[i: i + x_win_size]\n",
    "            y_win_data = tmp_data['Close'][i + x_win_size: i + x_win_size + y_win_size]\n",
    "\n",
    "            # to numpy array\n",
    "            x_win_arr = np.array(x_win_data)\n",
    "            y_win_arr = np.array(y_win_data)\n",
    "            x_inputs.append(x_win_arr)\n",
    "            y_inputs.append(y_win_arr)\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "        x_inputs = np.array(x_inputs)\n",
    "        y_inputs = np.array(y_inputs)\n",
    "        # reshape for plotting (_,)\n",
    "        y_inputs = np.reshape(y_inputs, (-1,))\n",
    "        print('Shape X:', np.shape(x_inputs), 'Shape Y:', np.shape(y_inputs))\n",
    "\n",
    "        return x_inputs, y_inputs, scaler\n",
    "\n",
    "    def create_inputs_zero_base(self, data, x_win_size=50, y_win_size=1):\n",
    "        # can store 2x in memory compared to float64\n",
    "        tmp_data = data.astype('float32')\n",
    "\n",
    "        self.drop_col(tmp_data, name='Open Time')\n",
    "        self.check_if_nan(tmp_data)\n",
    "\n",
    "        # BB_H, BB_L are in the range [0,1]\n",
    "        # RSI is oscillator [0, 100]\n",
    "        # -- Scale to [0, 2], then shift to [-1, 1] range\n",
    "        tmp_data[['RSI']] = ((tmp_data[['RSI']] / 100) * 2) - 1\n",
    "\n",
    "        x_inputs = []\n",
    "        y_inputs = []\n",
    "        close_bases = []\n",
    "        i = 0\n",
    "        while (i + x_win_size + y_win_size) <= len(tmp_data):\n",
    "            # create a copy to preserve original data\n",
    "            window_data = tmp_data[i: (i + x_win_size + y_win_size)].copy()\n",
    "            window_data, close_base = self.scale_zero_base(window_data)\n",
    "\n",
    "            # x[0:50] y[50:51]\n",
    "            x_win_data = window_data[: x_win_size]\n",
    "            y_win_data = window_data['Close'].iloc[-1]\n",
    "\n",
    "            # change to numpy array\n",
    "            x_win_arr = np.array(x_win_data)\n",
    "            x_inputs.append(x_win_arr)\n",
    "            y_inputs.append(y_win_data)\n",
    "            close_bases.append(close_base)\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "        x_inputs = np.array(x_inputs)\n",
    "        y_inputs = np.array(y_inputs)\n",
    "        close_bases = np.array(close_bases)\n",
    "\n",
    "        print('Shape X:', np.shape(x_inputs), 'Shape Y:', np.shape(y_inputs))\n",
    "        return x_inputs, y_inputs, close_bases\n",
    "\n",
    "    def create_inputs_reinforcement(self, data, x_win_size=10):\n",
    "        # can store 2x in memory compared to float64\n",
    "        tmp_data = data.astype('float32')\n",
    "\n",
    "        self.drop_col(tmp_data, name='Open Time')\n",
    "        self.check_if_nan(tmp_data)\n",
    "\n",
    "        # BB_H, BB_L are in the range [0,1]\n",
    "        # RSI is oscillator [0, 100] --> [0,1]\n",
    "        tmp_data[['RSI']] = tmp_data[['RSI']] / 100\n",
    "\n",
    "        scaler = self.scale_minmax(tmp_data)\n",
    "\n",
    "        # replace all 0, otherwise can cause division with 0\n",
    "        tmp_data['Close'].replace(0, method='bfill', inplace=True)\n",
    "\n",
    "        inputs = []\n",
    "        closing_prices = []\n",
    "        i = 0\n",
    "        while (i + x_win_size) <= len(tmp_data):\n",
    "            # e.g. x[0:50]\n",
    "            window = tmp_data[i: (i + x_win_size)]\n",
    "            # price of the window (1st element) at which trade is executed\n",
    "            close_price = window.loc[:, 'Close'].iloc[0]\n",
    "\n",
    "            # to numpy array\n",
    "            win_arr = np.array(window)\n",
    "            inputs.append(win_arr)\n",
    "            closing_prices.append(close_price)\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "        inputs = np.array(inputs)\n",
    "        closing_prices = np.array(closing_prices)\n",
    "\n",
    "        print('Shape Inputs:', np.shape(inputs))\n",
    "        return inputs, closing_prices, scaler\n",
    "\n",
    "    def scale_minmax(self, data):\n",
    "\n",
    "        # BB_H, BB_L, RSI are good\n",
    "        cols = [name for name in data.columns if name in ['BB_H', 'BB_L', 'RSI']]\n",
    "        tmp_df = pd.DataFrame()\n",
    "        for col in cols:\n",
    "            tmp_df[col] = data[col]\n",
    "\n",
    "        # rescale to [0, 1]\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "        scaler.fit_transform(data.values)\n",
    "\n",
    "        # replace BB_H, BB_L and RSI with original values\n",
    "        for col in cols:\n",
    "            data[col] = tmp_df[col]\n",
    "\n",
    "        return scaler\n",
    "\n",
    "    def scale_zero_base(self, data):\n",
    "\n",
    "        # closing price base for inverse scaling\n",
    "        close_base = data.loc[:, 'Close'].iloc[0]\n",
    "\n",
    "        # BB_H, BB_L, RSI are good\n",
    "        norm_cols = [name for name in data.columns if name not in ['BB_H', 'BB_L', 'RSI']]\n",
    "        for col in norm_cols:\n",
    "            # normalise against the 1st element for each window\n",
    "            tmp_base = data.loc[:, col].iloc[0]\n",
    "            data.loc[:, col] = (data.loc[:, col] / tmp_base) - 1\n",
    "\n",
    "        return data, close_base\n",
    "\n",
    "    def split_train_test(self, df, train_set_size=0.8):\n",
    "        train_set = df[:int(train_set_size * len(df))]\n",
    "        test_set = df[int(train_set_size * len(df)):]\n",
    "        return train_set, test_set\n",
    "\n",
    "\n",
    "p = PrepareData()\n",
    "\n",
    "# Extract and Process Data\n",
    "df_col = {}\n",
    "for sym in p.symbols:\n",
    "    for t in p.intervals:\n",
    "        print('\\nGetting data for {}, interval {}'.format(sym, t))\n",
    "        df, fname = p.extract_data(sym, t)\n",
    "\n",
    "        print('\\nProcessing data for {}, interval {}'.format(sym, t))\n",
    "        df_col[sym + '_' + t] = p.process_data(df, fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from plotly import tools as pytls\n",
    "\n",
    "\n",
    "def scale_plot(data_col):\n",
    "    # min-max scaling (values between 0 and 1)\n",
    "    scaled = (data_col - min(data_col)) / (max(data_col) - min(data_col))\n",
    "    return scaled\n",
    "\n",
    "\n",
    "def define_layout(plot_title, ty1, ty2):\n",
    "    layout = go.Layout(\n",
    "        title=plot_title,\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace'\n",
    "        ),\n",
    "        legend=dict(orientation='h'),\n",
    "        xaxis=dict(type='date'),\n",
    "        yaxis=dict(\n",
    "            domain=[0, 0.3],\n",
    "            title=ty1,\n",
    "            titlefont=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=16\n",
    "            ),\n",
    "            hoverformat='.4f',\n",
    "            tickformat='.f'\n",
    "        ),\n",
    "        yaxis2=dict(\n",
    "            domain=[0.4, 1],\n",
    "            title=ty2,\n",
    "            titlefont=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=16\n",
    "            ),\n",
    "            hoverformat='.8f',\n",
    "            tickformat='.6f'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return layout\n",
    "\n",
    "\n",
    "def define_results_layout(plot_title, x_label, y_label):\n",
    "    layout = go.Layout(\n",
    "        title=plot_title,\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace'\n",
    "        ),\n",
    "        legend=dict(orientation='h'),\n",
    "        xaxis=dict(\n",
    "            title=x_label,\n",
    "            titlefont=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=16\n",
    "            )\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=y_label,\n",
    "            titlefont=dict(\n",
    "                family='Courier New, monospace',\n",
    "                size=16\n",
    "            ),\n",
    "            hoverformat='.8f',\n",
    "            tickformat='.6f'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return layout\n",
    "\n",
    "\n",
    "def plot_individual(data, tag):\n",
    "    py.init_notebook_mode(connected=True)\n",
    "\n",
    "    trace_price = go.Scatter(\n",
    "        x=data['Open Time'],\n",
    "        y=data['Close'],\n",
    "        name=tag[:3]\n",
    "    )\n",
    "\n",
    "    trace_volume = go.Scatter(\n",
    "        x=data['Open Time'],\n",
    "        y=data['Volume'],\n",
    "        xaxis='x',\n",
    "        yaxis='y2',\n",
    "        name=tag[:3]\n",
    "    )\n",
    "\n",
    "    ty1 = 'Volume [BTC]'\n",
    "    ty2 = 'Closing Price [BTC]'\n",
    "    layout = define_layout(tag, ty1, ty2)\n",
    "\n",
    "    fig = pytls.make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.01)\n",
    "    fig.append_trace(trace_price, 2, 1)\n",
    "    fig.append_trace(trace_volume, 1, 1)\n",
    "\n",
    "    fig = go.Figure(fig, layout=layout)\n",
    "    py.iplot(fig, filename=tag)\n",
    "\n",
    "\n",
    "def plot_all(data, labels, normalise=False):\n",
    "    py.init_notebook_mode(connected=True)\n",
    "\n",
    "    fig = pytls.make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.01)\n",
    "    plot_title, ty1, ty2 = '', '', ''\n",
    "\n",
    "    colours = ['#1F77B4', '#B2182B', '#FF7F0E']\n",
    "    for i, key in enumerate(labels):\n",
    "        price_data = data[key]['Close']\n",
    "        vol_data = data[key]['Volume']\n",
    "        open_times = data[key]['Open Time']\n",
    "        plot_title = 'Comparison of Closing Prices and Volume of EOS, TRX and ONT'\n",
    "        ty1 = 'Volume [BTC]'\n",
    "        ty2 = 'Closing Price [BTC]'\n",
    "\n",
    "        if normalise:\n",
    "            price_data = scale_plot(price_data)\n",
    "            vol_data = scale_plot(vol_data)\n",
    "            plot_title = 'Comparison of Normalised Closing Prices and Volume of EOS, TRX and ONT'\n",
    "            ty1 = ty1[:-6]\n",
    "            ty2 = ty2[:-6]\n",
    "\n",
    "        tmp_price = go.Scatter(\n",
    "            x=open_times,\n",
    "            y=price_data,\n",
    "            name=key[:3],\n",
    "            line=dict(color=colours[i])\n",
    "        )\n",
    "        fig.append_trace(tmp_price, 2, 1)\n",
    "\n",
    "        tmp_vol = go.Scatter(\n",
    "            x=open_times,\n",
    "            y=vol_data,\n",
    "            xaxis='x',\n",
    "            yaxis='y2',\n",
    "            name=key[:3],\n",
    "            line=dict(color=colours[i]),\n",
    "            showlegend=False\n",
    "        )\n",
    "        fig.append_trace(tmp_vol, 1, 1)\n",
    "\n",
    "    layout = define_layout(plot_title, ty1, ty2)\n",
    "\n",
    "    fig = go.Figure(fig, layout=layout)\n",
    "    py.iplot(fig, filename='all-together')\n",
    "\n",
    "\n",
    "def plot_loss(history, tag):\n",
    "    py.init_notebook_mode(connected=True)\n",
    "\n",
    "    # Plot Loss\n",
    "    plot_title = 'Model Loss {}'.format(tag)\n",
    "    ty1 = 'Number of Epochs'\n",
    "    ty2 = 'Loss'\n",
    "\n",
    "    trace_loss = go.Scatter(\n",
    "        x=history.epoch,\n",
    "        y=history.history['loss'],\n",
    "        name='Loss'\n",
    "    )\n",
    "\n",
    "    trace_val_loss = go.Scatter(\n",
    "        x=history.epoch,\n",
    "        y=history.history['val_loss'],\n",
    "        name='Validation Loss'\n",
    "    )\n",
    "\n",
    "    layout = define_results_layout(plot_title, ty1, ty2)\n",
    "    data = [trace_loss, trace_val_loss]\n",
    "\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    py.iplot(fig, filename='model-loss')\n",
    "\n",
    "\n",
    "def plot_prediction(predicted_df, tag):\n",
    "    py.init_notebook_mode(connected=True)\n",
    "\n",
    "    # Plot Loss\n",
    "    plot_title = 'Prediction {}'.format(tag)\n",
    "    ty1 = 'Date'\n",
    "    ty2 = 'Price'\n",
    "\n",
    "    trace_target = go.Scatter(\n",
    "        x=predicted_df['Open Time'],\n",
    "        y=predicted_df['Target'],\n",
    "        name='Actual'\n",
    "    )\n",
    "\n",
    "    trace_predicted = go.Scatter(\n",
    "        x=predicted_df['Open Time'],\n",
    "        y=predicted_df['Results'],\n",
    "        name='Predicted'\n",
    "    )\n",
    "\n",
    "    layout = define_results_layout(plot_title, ty1, ty2)\n",
    "    # process timestamps as date\n",
    "    layout.update(xaxis=dict(type='date'))\n",
    "\n",
    "    data = [trace_target, trace_predicted]\n",
    "\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    py.iplot(fig, filename='prediction-results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        # load configuration file\n",
    "        configs = json.loads(open('config.json').read())\n",
    "\n",
    "        self.neurons = configs['model']['neurons']  # number of hidden units in the LSTM layer\n",
    "        self.activation_function = configs['model']['activation_function']\n",
    "        self.loss_function = configs['model']['loss_function']  # loss function for calculating the gradient\n",
    "        self.optimizer = configs['model']['optimizer']  # optimizer for applying gradient descent\n",
    "        self.dropout = configs['model']['dropout']  # dropout rate used after each LSTM layer to avoid overfitting\n",
    "        self.model_dir = configs['model']['models_dir']\n",
    "\n",
    "    def build_network(self, shape, output_size, reinforcement):\n",
    "        # start stacking layers\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(\n",
    "            self.neurons,\n",
    "            input_shape=(shape[1], shape[2]),  # Shape X (1105, 50, 9), Shape Y (1105, )\n",
    "            activation=self.activation_function\n",
    "        ))\n",
    "        model.add(Dropout(self.dropout))\n",
    "\n",
    "        model.add(Dense(units=output_size))  # 1 (price) or 3 (hold, long, short)\n",
    "        if not reinforcement:\n",
    "            model.add(Activation(self.activation_function))\n",
    "        else:\n",
    "            model.add(Activation('linear'))\n",
    "\n",
    "        model.compile(loss=self.loss_function, optimizer=self.optimizer)\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "def save_network(model, model_name, reinforcement):\n",
    "    # load configuration file\n",
    "    configs = json.loads(open('config.json').read())\n",
    "    model_dir = configs['model']['models_dir']\n",
    "    option = configs['reinforcement']['option']\n",
    "    if option == 1:\n",
    "        subdir = 'dqn'\n",
    "    elif option == 2:\n",
    "        subdir = 'ddqn'\n",
    "    else:\n",
    "        subdir = 'ddqnK'\n",
    "\n",
    "    fpath = path.join(model_dir, subdir) if reinforcement else model_dir\n",
    "    if not path.exists(fpath):\n",
    "        makedirs(fpath)\n",
    "\n",
    "    filename = path.join(fpath, model_name + '.h5')\n",
    "    model.save(filename)\n",
    "\n",
    "\n",
    "def load_network(model_name, reinforcement):\n",
    "    # load configuration file\n",
    "    configs = json.loads(open('config.json').read())\n",
    "    model_dir = configs['model']['models_dir']\n",
    "    option = configs['reinforcement']['option']\n",
    "    if option == 1:\n",
    "        subdir = 'dqn'\n",
    "    elif option == 2:\n",
    "        subdir = 'ddqn'\n",
    "    else:\n",
    "        subdir = 'ddqnK'\n",
    "\n",
    "    fpath = path.join(model_dir, subdir) if reinforcement else model_dir\n",
    "    filename = path.join(fpath, model_name + '.h5')\n",
    "    if not path.isfile(filename):\n",
    "        print('Model {} does not exist...'.format(model_name))\n",
    "        return False\n",
    "    else:\n",
    "        return load_model(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load configuration file\n",
    "configs = json.loads(open('config.json').read())\n",
    "x_window_size = configs['data']['x_window_size']\n",
    "y_window_size = configs['data']['y_window_size']\n",
    "input_scaling = configs['data']['input_scaling']\n",
    "train_set_size = configs['data']['train_set_size']\n",
    "batch_size = configs['model']['batch_size']  # default 32\n",
    "epochs = configs['model']['epochs']\n",
    "label = configs['data']['chart']\n",
    "\n",
    "p = PrepareData()\n",
    "chart = p.load_processed_data(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_raw_data(df_col):\n",
    "    key = 'EOSBTC_30m'\n",
    "    plot_individual(df_col[key], key)\n",
    "    key = 'TRXBTC_30m'\n",
    "    plot_individual(df_col[key], key)\n",
    "    key = 'ONTBTC_30m'\n",
    "    plot_individual(df_col[key], key)\n",
    "    labels = ['EOSBTC_30m', 'TRXBTC_30m', 'ONTBTC_30m']\n",
    "    plot_all(df_col, labels, normalise=True)\n",
    "    \n",
    "    \n",
    "plot_raw_data(df_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to training and testing data\n",
    "train_set, test_set = p.split_train_test(chart, train_set_size=train_set_size)\n",
    "\n",
    "if input_scaling == 'minmax':\n",
    "    # creates input data and labels for supervised learning\n",
    "    print('Generating training inputs and lables (X_train, Y_train)...')\n",
    "    X_train, Y_train, train_scaler = p.create_inputs_minmax(train_set, x_win_size=x_window_size, y_win_size=y_window_size)\n",
    "    # creates validation set to check for overfitting\n",
    "    print('Generating testing inputs and lables (X_test, Y_test)...')\n",
    "    X_test, Y_test, test_scaler = p.create_inputs_minmax(test_set, x_win_size=x_window_size, y_win_size=y_window_size)\n",
    "\n",
    "    model_name = label + '_e' + str(epochs) + '_minmax'\n",
    "\n",
    "else:\n",
    "    # creates input data and labels for supervised learning\n",
    "    print('Generating training inputs and lables (X_train, Y_train)...')\n",
    "    X_train, Y_train, train_bases = p.create_inputs_zero_base(train_set, x_win_size=x_window_size, y_win_size=y_window_size)\n",
    "    # creates validation set to check for overfitting\n",
    "    print('Generating testing inputs and lables (X_test, Y_test)...')\n",
    "    X_test, Y_test, test_bases = p.create_inputs_zero_base(test_set, x_win_size=x_window_size, y_win_size=y_window_size)\n",
    "\n",
    "    model_name = label + '_e' + str(epochs) + '_zerobase'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clean up memory\n",
    "gc.collect()\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(202)\n",
    "\n",
    "# create model architecture\n",
    "lstm = Model()\n",
    "lstm_model = lstm.build_network(shape=X_train.shape, output_size=1, reinforcement=False)\n",
    "\n",
    "history = lstm_model.fit(X_train, Y_train, epochs=epochs, shuffle=True, verbose=2, batch_size=batch_size, validation_data=(X_test, Y_test))\n",
    "\n",
    "# save the model to a file\n",
    "save_network(lstm_model, model_name, reinforcement=False)\n",
    "# plot model loss\n",
    "plot_loss(history, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def one_step_prediction(X_test, Y_test, chart, label, lstm_model):\n",
    "    predictions = lstm_model.predict(X_test, verbose=2)\n",
    "\n",
    "    # keep date for plotting\n",
    "    size_test_set = X_test.shape[0]\n",
    "    predicted_df = pd.DataFrame(chart['Open Time'].iloc[-size_test_set:])\n",
    "\n",
    "    # need to reset the index to start from 0\n",
    "    predicted_df.index = range(len(predicted_df))\n",
    "    predicted_df['Results'] = pd.DataFrame(predictions)\n",
    "    predicted_df['Target'] = pd.DataFrame(Y_test)\n",
    "\n",
    "    # plot prediction results\n",
    "    plot_prediction(predicted_df, label)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "single_step_p = one_step_prediction(X_test, Y_test, chart, label, lstm_model)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(single_step_p, Y_test))\n",
    "print('Single step RMSE: %.3f' % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def multi_step_prediction(lstm_model, p, inputs, x_window_size, y_window_size, input_scaling, label, iterations):\n",
    "    # 200 needed for MA_200 + 50 for window length\n",
    "    test_set = inputs[:250].copy()\n",
    "    # need to reset the index to start from 0\n",
    "    test_set.index = range(len(test_set))\n",
    "\n",
    "    # keep date for plotting\n",
    "    predicted_df = pd.DataFrame(inputs['Open Time'].iloc[250: (250 + iterations)])\n",
    "    predicted_df['Target'] = pd.DataFrame(inputs['Close'].iloc[250: (250 + iterations)])\n",
    "    predicted_df.index = range(len(predicted_df))\n",
    "\n",
    "    predictions = []\n",
    "    for i in range(iterations):\n",
    "        print('Iteration:', i + 1)\n",
    "\n",
    "        if input_scaling == 'minmax':\n",
    "            X_test, Y_test, scaler = p.create_inputs_minmax(test_set, x_win_size=x_window_size,\n",
    "                                                            y_win_size=y_window_size)\n",
    "        else:\n",
    "            X_test, Y_test, close_bases = p.create_inputs_zero_base(test_set, x_win_size=x_window_size,\n",
    "                                                                    y_win_size=y_window_size)\n",
    "\n",
    "        # only do prediction on the last window\n",
    "        last = X_test[-1]\n",
    "        last = np.reshape(last, (1, X_test.shape[1], X_test.shape[2]))\n",
    "        out = lstm_model.predict(last, verbose=2)\n",
    "\n",
    "        # ndarray with same number of features required for inverse scaling\n",
    "        tmp_nd = np.zeros((len(out), X_test.shape[2]))\n",
    "        # get Close column index (has to match for inverse scaling)\n",
    "        close_ix = test_set.columns.get_loc('Close')\n",
    "        # assign LSTM output to that column\n",
    "        tmp_nd[:, close_ix] = out\n",
    "\n",
    "        # perform inverse scaling and save the correct col value to out_inv\n",
    "        if input_scaling == 'minmax':\n",
    "            out_inv = scaler.inverse_transform(tmp_nd)[:, close_ix]\n",
    "        else:\n",
    "            out_inv = (out + 1) * close_bases[-1]\n",
    "\n",
    "        # change to float\n",
    "        out_inv = float(out_inv)\n",
    "\n",
    "        predictions.append(out_inv)\n",
    "        print('Predicted:', out_inv, 'True:', inputs.loc[:, 'Close'].iloc[i + 250])\n",
    "\n",
    "        # add output back to the initial df\n",
    "        test_set = test_set.append({'Close': out_inv}, ignore_index=True)\n",
    "\n",
    "        # calculate indicators\n",
    "        test_set = p.calculate_ta(test_set)\n",
    "\n",
    "    predicted_df['Results'] = pd.DataFrame(predictions)\n",
    "\n",
    "    # plot prediction results\n",
    "    plot_prediction(predicted_df, label)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "multi_step_p = multi_step_prediction(lstm_model, p, test_set, x_window_size, y_window_size, input_scaling, label, iterations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_shape):\n",
    "        # load configuration file\n",
    "        configs = json.loads(open('config.json').read())\n",
    "        self.action_size = configs['reinforcement']['action_size']  # hold, long. short\n",
    "        self.gamma = configs['reinforcement']['gamma']  # reward discount rate\n",
    "        self.epsilon = configs['reinforcement']['epsilon']  # exploration rate\n",
    "        self.epsilon_min = configs['reinforcement']['epsilon_min']\n",
    "        self.epsilon_decay = configs['reinforcement']['epsilon_decay']\n",
    "        self.batch_size = configs['reinforcement']['batch_size']\n",
    "\n",
    "        memory_size = configs['reinforcement']['memory_size']\n",
    "        self.memory = deque(maxlen=memory_size)  # deque automatically removes oldest memory once the maxlen is reached\n",
    "        self.state_shape = state_shape[1:]  # (_, window_len, features)\n",
    "        self.inventory = []\n",
    "        self.trades = []\n",
    "        self.history = 0\n",
    "\n",
    "        m = Model()\n",
    "        self.model = m.build_network(shape=state_shape, output_size=self.action_size, reinforcement=True)\n",
    "        # implementation of double dqn\n",
    "        self.target_model = m.build_network(shape=state_shape, output_size=self.action_size, reinforcement=True)\n",
    "\n",
    "    def compute_action(self, state, evaluation, explore_p):\n",
    "        # get random action based on epsilon for exploration\n",
    "        if not evaluation and explore_p > np.random.rand():\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        # returns a list\n",
    "        options = self.model.predict(state)\n",
    "        # returns the index of the max value in this state [_, _, _]\n",
    "        return np.argmax(options[0])  # returns action\n",
    "\n",
    "    def calculate_reward(self, action, price, long, short):\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1:  # buy\n",
    "\n",
    "            # if new trade\n",
    "            if not long and not short:\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # if already short\n",
    "            elif short:\n",
    "                # close short, open long\n",
    "                sold_price = self.inventory.pop()\n",
    "                # short is inverse\n",
    "                reward = (sold_price - price) / sold_price  # reward is percentage\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # set new trade\n",
    "            long = True\n",
    "            short = False\n",
    "\n",
    "        elif action == 2:  # sell\n",
    "\n",
    "            # if new trade\n",
    "            if not long and not short:\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # if already long\n",
    "            elif long:\n",
    "                # close long, open short\n",
    "                bought_price = self.inventory.pop()\n",
    "                reward = (price - bought_price) / bought_price\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # set new trade\n",
    "            long = False\n",
    "            short = True\n",
    "\n",
    "        profit = reward * 100  # in percentage\n",
    "\n",
    "        # clipping reward\n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        return reward, profit, long, short\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def experience_replay(self, batch_size):\n",
    "\n",
    "        # fill minibatch with random states from the memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        states = []\n",
    "        targets = []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # predict Q values for current state [[_, _, _]]\n",
    "            target = self.model.predict(state)\n",
    "\n",
    "            if done:\n",
    "                Q_target = reward\n",
    "            else:\n",
    "                # for less variation use target model to predict the Q values and pick max\n",
    "                maxQ = np.amax(self.target_model.predict(next_state)[0])\n",
    "                ''' Q(s,a) = r(s,a) + y*max(Q(s',a))\n",
    "                Q target is reward of taking action at current state plus discounted max Q of all possible\n",
    "                actions from the next state '''\n",
    "                Q_target = reward + self.gamma * maxQ\n",
    "\n",
    "            # replace Q value of action with best Q value for actions from next state\n",
    "            target[0][action] = Q_target\n",
    "\n",
    "            state = np.reshape(state, (10, 9))\n",
    "            target = np.reshape(target, 3)\n",
    "            states.append(state)\n",
    "            targets.append(target)\n",
    "\n",
    "        # train the model towards updated prediction\n",
    "        history = self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "        # save loss history\n",
    "        self.history = history\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # load configuration file\n",
    "    configs = json.loads(open('config.json').read())\n",
    "\n",
    "    train_set_size = configs['data']['train_set_size']\n",
    "    label = configs['data']['chart']\n",
    "    window_size = configs['reinforcement']['window_size']\n",
    "    episodes = configs['reinforcement']['episodes']\n",
    "    option = configs['reinforcement']['option']\n",
    "\n",
    "    df = p.load_processed_data(label)\n",
    "    train_set, _ = p.split_train_test(df, train_set_size=train_set_size)\n",
    "\n",
    "    # # trend test\n",
    "    # df = p.load_processed_data('ONTBTC_15m')\n",
    "    # up_trend = df[:5062]\n",
    "    # down_trend = df[5062:]\n",
    "    # train_set_up, test_set_up = p.split_train_test(up_trend, train_set_size=train_set_size)\n",
    "    # train_set_down, test_set_down = p.split_train_test(down_trend, train_set_size=train_set_size)\n",
    "\n",
    "    print('Generating inputs ...')\n",
    "    inputs, closing_prices, scaler = p.create_inputs_reinforcement(train_set, x_win_size=window_size)\n",
    "    # inputs, closing_prices, scaler = p.create_inputs_reinforcement(train_set_up, x_win_size=window_size)\n",
    "    \n",
    "    agent = DQNAgent(np.shape(inputs))\n",
    "    name = 'DQN'\n",
    "    \n",
    "    # network = model.load_network('{}_ep{}'.format(label, 60), reinforcement=True)\n",
    "    # if network is not False:\n",
    "    #     agent.model = network\n",
    "\n",
    "    # update weights of target network with dqn network weights\n",
    "    agent.update_target_model()\n",
    "    evaluation = False  # switch on at testing stage\n",
    "\n",
    "    rewards = []\n",
    "    losses = []\n",
    "    decay_step = 0\n",
    "\n",
    "    time_steps = len(inputs) - 1\n",
    "    print('------------------------------- {} --- {} ----------------------------'.format(name, label))\n",
    "    for e in range(episodes + 1):\n",
    "        # reset the environment (first window [0:10])\n",
    "        state = inputs[0]\n",
    "        state = np.reshape(state, (1, agent.state_shape[0], agent.state_shape[1]))  # (1, 10, 9)\n",
    "        total_reward = 0\n",
    "        total_profit = 0\n",
    "        long = False\n",
    "        short = False\n",
    "        action = 0\n",
    "        agent.inventory = []\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            # print('Step {}/{}'.format(t, time_steps))\n",
    "            # history = agent.history.history if not agent.history == 0 else 0\n",
    "            # print('Agent\\'s inventory:', agent.inventory, 'Action:', action, 'Total profit:', total_profit, 'Loss:', history)\n",
    "\n",
    "            decay_step += 1\n",
    "            # calculate exploration probability\n",
    "            explore_p = agent.epsilon_min + (agent.epsilon - agent.epsilon_min) * np.exp(\n",
    "                -agent.epsilon_decay * decay_step)\n",
    "\n",
    "            # compute action based on current state\n",
    "            action = agent.compute_action(state, evaluation, explore_p)\n",
    "\n",
    "            # calculate reward\n",
    "            reward, profit, long, short = agent.calculate_reward(action, closing_prices[t], long, short)\n",
    "            total_reward += reward\n",
    "            total_profit += profit\n",
    "            # print('Reward\\t:', reward, 'Profit:', profit)\n",
    "            # print('T Reward:', total_reward, 'T Profit:', total_profit)\n",
    "\n",
    "            done = True if t == time_steps - 1 else False\n",
    "\n",
    "            if done:\n",
    "                rewards.append(total_reward)\n",
    "                losses.append(agent.history.history)\n",
    "\n",
    "                agent.update_target_model()\n",
    "\n",
    "                history = agent.history.history if not agent.history == 0 else 0\n",
    "                print(\"Episode: {}/{}\\tTotal Reward: {}\\tTotal Profit: {}\\tLoss: {}\".format(e, episodes, total_reward,\n",
    "                                                                                            total_profit, history))\n",
    "                break\n",
    "\n",
    "            # get next state\n",
    "            next_state = inputs[t + 1]\n",
    "            next_state = np.reshape(next_state, state.shape)\n",
    "\n",
    "            # store in memory (SARS')\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            # recall after memory has enough samples in it\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                agent.experience_replay(agent.batch_size)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            model.save_network(agent.model, '{}_ep{}'.format(label, e), reinforcement=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "\n",
    "    def __init__(self, state_shape):\n",
    "        # load configuration file\n",
    "        configs = json.loads(open('config.json').read())\n",
    "        self.action_size = configs['reinforcement']['action_size']  # hold, long. short\n",
    "        self.gamma = configs['reinforcement']['gamma']  # reward discount rate\n",
    "        self.epsilon = configs['reinforcement']['epsilon']  # exploration rate\n",
    "        self.epsilon_min = configs['reinforcement']['epsilon_min']\n",
    "        self.epsilon_decay = configs['reinforcement']['epsilon_decay']\n",
    "        self.batch_size = configs['reinforcement']['batch_size']\n",
    "\n",
    "        memory_size = configs['reinforcement']['memory_size']\n",
    "        self.memory = deque(maxlen=memory_size)  # deque automatically removes oldest memory once the maxlen is reached\n",
    "        self.state_shape = state_shape[1:]  # (_, window_len, features)\n",
    "        self.inventory = []\n",
    "        self.trades = []\n",
    "        self.history = 0\n",
    "\n",
    "        m = Model()\n",
    "        self.model = m.build_network(shape=state_shape, output_size=self.action_size, reinforcement=True)\n",
    "        # implementation of double dqn\n",
    "        self.target_model = m.build_network(shape=state_shape, output_size=self.action_size, reinforcement=True)\n",
    "\n",
    "    def compute_action(self, state, evaluation, explore_p):\n",
    "        # get random action based on epsilon for exploration\n",
    "        if not evaluation and explore_p > np.random.rand():\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        # returns a list\n",
    "        options = self.model.predict(state)\n",
    "        # returns the index of the max value in this state [_, _, _]\n",
    "        return np.argmax(options[0])  # returns action\n",
    "\n",
    "    def calculate_reward(self, action, price, long, short):\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1:  # buy\n",
    "\n",
    "            # if new trade\n",
    "            if not long and not short:\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # if already short\n",
    "            elif short:\n",
    "                # close short, open long\n",
    "                sold_price = self.inventory.pop()\n",
    "                # short is inverse\n",
    "                reward = (sold_price - price) / sold_price  # reward is percentage\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # set new trade\n",
    "            long = True\n",
    "            short = False\n",
    "\n",
    "        elif action == 2:  # sell\n",
    "\n",
    "            # if new trade\n",
    "            if not long and not short:\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # if already long\n",
    "            elif long:\n",
    "                # close long, open short\n",
    "                bought_price = self.inventory.pop()\n",
    "                reward = (price - bought_price) / bought_price\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # set new trade\n",
    "            long = False\n",
    "            short = True\n",
    "\n",
    "        profit = reward * 100  # in percentage\n",
    "\n",
    "        # clipping reward\n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        return reward, profit, long, short\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def experience_replay(self, batch_size):\n",
    "\n",
    "        # fill minibatch with random states from the memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        states = []\n",
    "        targets = []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # predict Q values for current state [[_, _, _]]\n",
    "            target = self.model.predict(state)\n",
    "\n",
    "            q_next_state = self.model.predict(next_state)\n",
    "            q_target_next_state = self.target_model.predict(next_state)\n",
    "            a = np.argmax(q_next_state[0])\n",
    "\n",
    "            if done:\n",
    "                Q_target = reward\n",
    "            else:\n",
    "                # Q(s,a) = r(s,a) + y*Q(s',argmax(Q(s',a)\n",
    "                Q_target = reward + self.gamma * q_target_next_state[0][a]\n",
    "\n",
    "            # replace Q value of action with best Q value for actions from next state\n",
    "            target[0][action] = Q_target\n",
    "\n",
    "            state = np.reshape(state, (10, 9))\n",
    "            target = np.reshape(target, 3)\n",
    "            states.append(state)\n",
    "            targets.append(target)\n",
    "\n",
    "        # train the model towards updated prediction\n",
    "        history = self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "        # save loss history\n",
    "        self.history = history\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # load configuration file\n",
    "    configs = json.loads(open('config.json').read())\n",
    "\n",
    "    train_set_size = configs['data']['train_set_size']\n",
    "    label = configs['data']['chart']\n",
    "    window_size = configs['reinforcement']['window_size']\n",
    "    episodes = configs['reinforcement']['episodes']\n",
    "    option = configs['reinforcement']['option']\n",
    "\n",
    "    df = p.load_processed_data(label)\n",
    "    train_set, _ = p.split_train_test(df, train_set_size=train_set_size)\n",
    "\n",
    "    # # trend test\n",
    "    # df = p.load_processed_data('ONTBTC_15m')\n",
    "    # up_trend = df[:5062]\n",
    "    # down_trend = df[5062:]\n",
    "    # train_set_up, test_set_up = p.split_train_test(up_trend, train_set_size=train_set_size)\n",
    "    # train_set_down, test_set_down = p.split_train_test(down_trend, train_set_size=train_set_size)\n",
    "\n",
    "    print('Generating inputs ...')\n",
    "    inputs, closing_prices, scaler = p.create_inputs_reinforcement(train_set, x_win_size=window_size)\n",
    "    # inputs, closing_prices, scaler = p.create_inputs_reinforcement(train_set_up, x_win_size=window_size)\n",
    "    \n",
    "    agent = DDQNAgent(np.shape(inputs))\n",
    "    name = 'DDQN'\n",
    "    \n",
    "    # network = model.load_network('{}_ep{}'.format(label, 60), reinforcement=True)\n",
    "    # if network is not False:\n",
    "    #     agent.model = network\n",
    "\n",
    "    # update weights of target network with dqn network weights\n",
    "    agent.update_target_model()\n",
    "    evaluation = False  # switch on at testing stage\n",
    "\n",
    "    rewards = []\n",
    "    losses = []\n",
    "    decay_step = 0\n",
    "\n",
    "    time_steps = len(inputs) - 1\n",
    "    print('------------------------------- {} --- {} ----------------------------'.format(name, label))\n",
    "    for e in range(episodes + 1):\n",
    "        # reset the environment (first window [0:10])\n",
    "        state = inputs[0]\n",
    "        state = np.reshape(state, (1, agent.state_shape[0], agent.state_shape[1]))  # (1, 10, 9)\n",
    "        total_reward = 0\n",
    "        total_profit = 0\n",
    "        long = False\n",
    "        short = False\n",
    "        action = 0\n",
    "        agent.inventory = []\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            # print('Step {}/{}'.format(t, time_steps))\n",
    "            # history = agent.history.history if not agent.history == 0 else 0\n",
    "            # print('Agent\\'s inventory:', agent.inventory, 'Action:', action, 'Total profit:', total_profit, 'Loss:', history)\n",
    "\n",
    "            decay_step += 1\n",
    "            # calculate exploration probability\n",
    "            explore_p = agent.epsilon_min + (agent.epsilon - agent.epsilon_min) * np.exp(\n",
    "                -agent.epsilon_decay * decay_step)\n",
    "\n",
    "            # compute action based on current state\n",
    "            action = agent.compute_action(state, evaluation, explore_p)\n",
    "\n",
    "            # calculate reward\n",
    "            reward, profit, long, short = agent.calculate_reward(action, closing_prices[t], long, short)\n",
    "            total_reward += reward\n",
    "            total_profit += profit\n",
    "            # print('Reward\\t:', reward, 'Profit:', profit)\n",
    "            # print('T Reward:', total_reward, 'T Profit:', total_profit)\n",
    "\n",
    "            done = True if t == time_steps - 1 else False\n",
    "\n",
    "            if done:\n",
    "                rewards.append(total_reward)\n",
    "                losses.append(agent.history.history)\n",
    "\n",
    "                agent.update_target_model()\n",
    "\n",
    "                history = agent.history.history if not agent.history == 0 else 0\n",
    "                print(\"Episode: {}/{}\\tTotal Reward: {}\\tTotal Profit: {}\\tLoss: {}\".format(e, episodes, total_reward,\n",
    "                                                                                            total_profit, history))\n",
    "                break\n",
    "\n",
    "            # get next state\n",
    "            next_state = inputs[t + 1]\n",
    "            next_state = np.reshape(next_state, state.shape)\n",
    "\n",
    "            # store in memory (SARS')\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            # recall after memory has enough samples in it\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                agent.experience_replay(agent.batch_size)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            model.save_network(agent.model, '{}_ep{}'.format(label, e), reinforcement=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN by Keon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNKAgent:\n",
    "\n",
    "    def __init__(self, state_shape):\n",
    "        # load configuration file\n",
    "        configs = json.loads(open('config.json').read())\n",
    "        self.action_size = configs['reinforcement']['action_size']  # hold, long. short\n",
    "        self.gamma = configs['reinforcement']['gamma']  # reward discount rate\n",
    "        self.epsilon = configs['reinforcement']['epsilon']  # exploration rate\n",
    "        self.epsilon_min = configs['reinforcement']['epsilon_min']\n",
    "        self.epsilon_decay = configs['reinforcement']['epsilon_decay']\n",
    "        self.batch_size = configs['reinforcement']['batch_size']\n",
    "\n",
    "        memory_size = configs['reinforcement']['memory_size']\n",
    "        self.memory = deque(maxlen=memory_size)  # deque automatically removes oldest memory once the maxlen is reached\n",
    "        self.state_shape = state_shape[1:]  # (_, window_len, features)\n",
    "        self.inventory = []\n",
    "        self.trades = []\n",
    "        self.history = 0\n",
    "\n",
    "        m = Model()\n",
    "        self.model = m.build_network(shape=state_shape, output_size=self.action_size, reinforcement=True)\n",
    "        # implementation of double dqn\n",
    "        self.target_model = m.build_network(shape=state_shape, output_size=self.action_size, reinforcement=True)\n",
    "\n",
    "    def compute_action(self, state, evaluation, explore_p):\n",
    "        # get random action based on epsilon for exploration\n",
    "        if not evaluation and explore_p > np.random.rand():\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        # returns a list\n",
    "        options = self.model.predict(state)\n",
    "        # returns the index of the max value in this state [_, _, _]\n",
    "        return np.argmax(options[0])  # returns action\n",
    "\n",
    "    def calculate_reward(self, action, price, long, short):\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1:  # buy\n",
    "\n",
    "            # if new trade\n",
    "            if not long and not short:\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # if already short\n",
    "            elif short:\n",
    "                # close short, open long\n",
    "                sold_price = self.inventory.pop()\n",
    "                # short is inverse\n",
    "                reward = (sold_price - price) / sold_price  # reward is percentage\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # set new trade\n",
    "            long = True\n",
    "            short = False\n",
    "\n",
    "        elif action == 2:  # sell\n",
    "\n",
    "            # if new trade\n",
    "            if not long and not short:\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # if already long\n",
    "            elif long:\n",
    "                # close long, open short\n",
    "                bought_price = self.inventory.pop()\n",
    "                reward = (price - bought_price) / bought_price\n",
    "                self.inventory.append(price)\n",
    "\n",
    "            # set new trade\n",
    "            long = False\n",
    "            short = True\n",
    "\n",
    "        profit = reward * 100  # in percentage\n",
    "\n",
    "        # clipping reward\n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        return reward, profit, long, short\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def experience_replay(self, batch_size):\n",
    "\n",
    "        # fill minibatch with random states from the memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        states = []\n",
    "        targets = []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # predict Q values for current state [[_, _, _]]\n",
    "            target = self.model.predict(state)\n",
    "\n",
    "            q_target_next_state = self.target_model.predict(next_state)\n",
    "\n",
    "            if done:\n",
    "                Q_target = reward\n",
    "            else:\n",
    "                # Q(s,a) = r(s,a) + y*Q(s',argmax(Q(s',a)\n",
    "                Q_target = reward + self.gamma * np.amax(q_target_next_state[0])\n",
    "\n",
    "            # replace Q value of action with best Q value for actions from next state\n",
    "            target[0][action] = Q_target\n",
    "\n",
    "            state = np.reshape(state, (10, 9))\n",
    "            target = np.reshape(target, 3)\n",
    "            states.append(state)\n",
    "            targets.append(target)\n",
    "\n",
    "            # train the model towards updated prediction\n",
    "        history = self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "        # save loss history\n",
    "        self.history = history\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # load configuration file\n",
    "    configs = json.loads(open('config.json').read())\n",
    "\n",
    "    train_set_size = configs['data']['train_set_size']\n",
    "    label = configs['data']['chart']\n",
    "    window_size = configs['reinforcement']['window_size']\n",
    "    episodes = configs['reinforcement']['episodes']\n",
    "    option = configs['reinforcement']['option']\n",
    "\n",
    "    df = p.load_processed_data(label)\n",
    "    train_set, _ = p.split_train_test(df, train_set_size=train_set_size)\n",
    "\n",
    "    # # trend test\n",
    "    # df = p.load_processed_data('ONTBTC_15m')\n",
    "    # up_trend = df[:5062]\n",
    "    # down_trend = df[5062:]\n",
    "    # train_set_up, test_set_up = p.split_train_test(up_trend, train_set_size=train_set_size)\n",
    "    # train_set_down, test_set_down = p.split_train_test(down_trend, train_set_size=train_set_size)\n",
    "\n",
    "    print('Generating inputs ...')\n",
    "    inputs, closing_prices, scaler = p.create_inputs_reinforcement(train_set, x_win_size=window_size)\n",
    "    # inputs, closing_prices, scaler = p.create_inputs_reinforcement(train_set_up, x_win_size=window_size)\n",
    "    \n",
    "    agent = DDQNKAgent(np.shape(inputs))\n",
    "    name = 'DDQN by Keon'\n",
    "    \n",
    "    # network = model.load_network('{}_ep{}'.format(label, 60), reinforcement=True)\n",
    "    # if network is not False:\n",
    "    #     agent.model = network\n",
    "\n",
    "    # update weights of target network with dqn network weights\n",
    "    agent.update_target_model()\n",
    "    evaluation = False  # switch on at testing stage\n",
    "\n",
    "    rewards = []\n",
    "    losses = []\n",
    "    decay_step = 0\n",
    "\n",
    "    time_steps = len(inputs) - 1\n",
    "    print('------------------------------- {} --- {} ----------------------------'.format(name, label))\n",
    "    for e in range(episodes + 1):\n",
    "        # reset the environment (first window [0:10])\n",
    "        state = inputs[0]\n",
    "        state = np.reshape(state, (1, agent.state_shape[0], agent.state_shape[1]))  # (1, 10, 9)\n",
    "        total_reward = 0\n",
    "        total_profit = 0\n",
    "        long = False\n",
    "        short = False\n",
    "        action = 0\n",
    "        agent.inventory = []\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            # print('Step {}/{}'.format(t, time_steps))\n",
    "            # history = agent.history.history if not agent.history == 0 else 0\n",
    "            # print('Agent\\'s inventory:', agent.inventory, 'Action:', action, 'Total profit:', total_profit, 'Loss:', history)\n",
    "\n",
    "            decay_step += 1\n",
    "            # calculate exploration probability\n",
    "            explore_p = agent.epsilon_min + (agent.epsilon - agent.epsilon_min) * np.exp(\n",
    "                -agent.epsilon_decay * decay_step)\n",
    "\n",
    "            # compute action based on current state\n",
    "            action = agent.compute_action(state, evaluation, explore_p)\n",
    "\n",
    "            # calculate reward\n",
    "            reward, profit, long, short = agent.calculate_reward(action, closing_prices[t], long, short)\n",
    "            total_reward += reward\n",
    "            total_profit += profit\n",
    "            # print('Reward\\t:', reward, 'Profit:', profit)\n",
    "            # print('T Reward:', total_reward, 'T Profit:', total_profit)\n",
    "\n",
    "            done = True if t == time_steps - 1 else False\n",
    "\n",
    "            if done:\n",
    "                rewards.append(total_reward)\n",
    "                losses.append(agent.history.history)\n",
    "\n",
    "                agent.update_target_model()\n",
    "\n",
    "                history = agent.history.history if not agent.history == 0 else 0\n",
    "                print(\"Episode: {}/{}\\tTotal Reward: {}\\tTotal Profit: {}\\tLoss: {}\".format(e, episodes, total_reward,\n",
    "                                                                                            total_profit, history))\n",
    "                break\n",
    "\n",
    "            # get next state\n",
    "            next_state = inputs[t + 1]\n",
    "            next_state = np.reshape(next_state, state.shape)\n",
    "\n",
    "            # store in memory (SARS')\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            # recall after memory has enough samples in it\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                agent.experience_replay(agent.batch_size)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            model.save_network(agent.model, '{}_ep{}'.format(label, e), reinforcement=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
